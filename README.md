# EvalAItoNomartiveConsults

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1HThgioW620LWUm4G6vCqE17_tcxtsdUf?usp=sharing)

Uma ferramenta completa para avaliaÃ§Ã£o comparativa de modelos de IA em consultas jurÃ­dicas brasileiras. O sistema automatiza o pipeline RAG (Retrieval-Augmented Generation): gera queries de busca, coleta contextos legais da LexML, produz respostas e avalia mÃºltiplas mÃ©tricas de qualidade.

## ğŸš€ Recursos Principais

- **AvaliaÃ§Ã£o AutomÃ¡tica**: Compare modelos de IA em tarefas jurÃ­dicas brasileiras.
- **MÃ©tricas Abrangentes**: Faithfulness, Relevancy, Context Precision, ROUGE, BERTScore.
- **EstratÃ©gias de Contexto**: Truncamento regressivo ou resumo inteligente para limites de tokens.
- **Interface Web**: FÃ¡cil de usar via Streamlit, sem necessidade de cÃ³digo.
- **Flexibilidade**: Modelos personalizados, perguntas customizadas, ground truths opcionais.
- **RelatÃ³rios Detalhados**: Resultados em JSON, CSV e rankings comparativos.

## ğŸ“‹ PrÃ©-requisitos

- **Python 3.8+** instalado.
- **Chave de API do OpenRouter** (gratuita para testes, paga para uso intensivo).
- **ConexÃ£o com internet** para acesso aos modelos e busca de contextos.

## ğŸ› ï¸ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/expagepay/EvalAItoNomartiveConsults.git
   cd EvalAItoNomartiveConsults
   ```

2. **Configure o ambiente**:
   - Use o launcher da interface web (cria venv automaticamente):
     ```bash
     cd web_interface
     python launcher.py
     ```
     Isso instala `requirements.txt` e configura o ambiente.

3. **Configure a API**:
   - Crie `.env` na raiz:
     ```
     OPENAI_API_KEY=sk-or-v1-sua-chave-openrouter
     ```
   - Obtenha em [OpenRouter Keys](https://openrouter.ai/keys).

4. **Configure cache (opcional, acelera inicializaÃ§Ãµes)**:
   - Adicione ao `.env`:
     ```
     HF_HUB_CACHE=./HF_Cache
     ```

## ğŸ¯ Uso BÃ¡sico

### Interface Web (Recomendado)

```bash
cd web_interface
python launcher.py
```
Acesse `http://localhost:8501`. Configure perguntas, modelos e execute avaliaÃ§Ãµes interativamente.

### CLI (Linha de Comando)

```bash
# AvaliaÃ§Ã£o rÃ¡pida com perguntas padrÃ£o
python run.py --quick_eval

# Com perguntas customizadas
python run.py --perguntas "O que Ã© LGPD?" "Direitos do consumidor"

# Com arquivo CSV (colunas: pergunta, ground_truth)
python run.py --csv_file meu_arquivo.csv

# Ver opÃ§Ãµes completas
python run.py --help
```

### Notebook Colab

Abra o [notebook Colab](https://colab.research.google.com/drive/1HThgioW620LWUm4G6vCqE17_tcxtsdUf?usp=sharing) para execuÃ§Ã£o na nuvem.

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Modelos Personalizados

Use qualquer modelo do [OpenRouter](https://openrouter.ai/models). Exemplos:

```bash
# Modelos padrÃ£o
python run.py --modelos meta-llama/llama-3.3-70b-instruct mistralai/mistral-7b-instruct

# Modelos customizados
python run.py --modelos openai/gpt-4o anthropic/claude-3.5-sonnet google/gemini-pro-1.5
```

**Dicas**:
- Modelos maiores (>100k tokens) sÃ£o melhores para contexto jurÃ­dico.
- Teste limites: GPT-4 ~128k, Claude 3 ~200k, Gemini 1.5 ~1M.
- Custos variam; verifique no OpenRouter.

### NÃºmero de Queries (`num_queries`)

Controla quantas queries de busca o modelo gera por pergunta.

```bash
# PadrÃ£o: 3 queries
python run.py --num_queries 5  # Mais queries = mais contexto, mas mais lento
```

**RecomendaÃ§Ã£o**: 3-5 para equilÃ­brio entre qualidade e velocidade.

### Modo de Tratamento de Contexto (`modo_contexto`)

Quando o contexto excede limites de tokens, escolha a estratÃ©gia:

- **`truncar`** (padrÃ£o): Reduz regressivamente (700k â†’ 100k â†’ 50k â†’ 28k chars).
- **`resumir`**: Gera resumo com Gemini 2.5 Flash, preservando essÃªncia.

```bash
# Truncamento regressivo
python run.py --modo_contexto truncar

# Resumo inteligente
python run.py --modo_contexto resumir
```

**Quando usar**:
- `truncar`: RÃ¡pido, preserva original.
- `resumir`: Melhor para contextos muito grandes, mas usa tokens extras.

### System Prompts Personalizados

Personalize comportamento dos modelos:

```bash
# Via arquivo
python run.py --system_queries_file meu_prompt_queries.txt --system_resposta_file meu_prompt_resposta.txt

# Ou diretamente (use com aspas)
python run.py --system_queries "VocÃª Ã© um especialista jurÃ­dico..."
```

### Ground Truths

ForneÃ§a respostas ideais para mÃ©tricas mais precisas:

```bash
python run.py --perguntas "O que Ã© LGPD?" --ground_truth "Lei Geral de ProteÃ§Ã£o de Dados (Lei nÂº 13.709/2018)"
```

Sem ground truth, o sistema gera automaticamente (aproximado).

## ğŸ“Š MÃ©tricas Explicadas

O sistema avalia respostas com mÃ©tricas RAGAS + textuais:

### MÃ©tricas RAGAS (Avaliam Qualidade RAG)
- **Faithfulness (Fidelidade)**: Resposta consistente com contexto? (0-1, alto = fiel).
- **Answer Relevancy (RelevÃ¢ncia)**: Resposta relevante para pergunta? (0-1, alto = relevante).
- **Context Precision (PrecisÃ£o do Contexto)**: Contextos recuperados sÃ£o relevantes/Ãºteis? (0-1, alto = precisos).

### MÃ©tricas Textuais (ComparaÃ§Ã£o com Ground Truth)
- **ROUGE-1/2**: Similaridade n-gram (0-1, alto = similar).
- **BERTScore**: Similaridade semÃ¢ntica via embeddings (0-1, alto = prÃ³ximo).

### InterpretaÃ§Ã£o
- **Alto em tudo**: Resposta excelente, bem fundamentada.
- **Baixo Faithfulness**: Modelo "inventou" info.
- **Baixo Relevancy**: Resposta off-topic.
- **Baixo Context Precision**: Busca ruim, contextos irrelevantes.

## ğŸŒ Interface Web Detalhada

Execute `python launcher.py` na pasta `web_interface`.

### Funcionalidades
- **SeleÃ§Ã£o de Modelos**: Escolha de lista prÃ©-definida ou customizada.
- **Perguntas**: Texto livre ou upload CSV.
- **ConfiguraÃ§Ãµes**: num_queries, modo_contexto, system prompts.
- **Resultados**: Tabela comparativa, issues identificados, downloads.

### Issues Identificados
A interface mostra problemas como:
- "Queries JSON malformado"
- "Nenhum contexto recuperado"
- "Resposta vazia"

Ajuda depurar execuÃ§Ãµes.

## ğŸ“ Estrutura de Arquivos

```
EvalAItoNomartiveConsults/
â”œâ”€â”€ main.py              # Pipeline principal
â”œâ”€â”€ models.py            # GeraÃ§Ã£o de queries/respostas
â”œâ”€â”€ metrics.py           # AvaliaÃ§Ã£o de mÃ©tricas
â”œâ”€â”€ retriever.py         # Busca de contextos LexML
â”œâ”€â”€ report.py            # GeraÃ§Ã£o de relatÃ³rios
â”œâ”€â”€ run.py               # CLI
â”œâ”€â”€ web_interface/       # Interface Streamlit
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ launcher.py
â”œâ”€â”€ results/             # SaÃ­das (JSON, CSV)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                 # ConfiguraÃ§Ãµes (nÃ£o versionado)
â””â”€â”€ README.md
```

## ğŸ”§ SoluÃ§Ã£o de Problemas

### InicializaÃ§Ã£o Lenta
- **Causa**: Carregamento de embeddings na primeira avaliaÃ§Ã£o.
- **SoluÃ§Ã£o**: Aguarde; prÃ³ximas execuÃ§Ãµes sÃ£o rÃ¡pidas (cache).

### Erro de API
- Verifique `OPENAI_API_KEY` no `.env`.
- CrÃ©ditos insuficientes? Recarregue no OpenRouter.

### Contextos Vazios
- Queries ruins? Verifique issues.
- Rede? Teste conectividade com LexML.

### Modelo NÃ£o Responde
- Limite de tokens? Use `modo_contexto truncar`.
- Erro de parsing? Sistema tem fallbacks.

### Cache de Modelos
- Delete `HF_Cache/` para forÃ§ar re-download se corrompido.

## ğŸ“ˆ Exemplos de Uso

### ComparaÃ§Ã£o BÃ¡sica
```bash
python run.py --quick_eval --modelos meta-llama/llama-3.3-70b-instruct openai/gpt-3.5-turbo
```

### AvaliaÃ§Ã£o JurÃ­dica Detalhada
```bash
python run.py \
  --perguntas "Quais sÃ£o os direitos trabalhistas no Brasil?" \
  --ground_truth "CLT prevÃª jornada de 8h, fÃ©rias, etc." \
  --num_queries 5 \
  --modo_contexto resumir \
  --modelos anthropic/claude-3.5-sonnet
```

### RelatÃ³rio Final
ApÃ³s execuÃ§Ã£o, veja `results/`:
- `resultados.json`: Dados brutos.
- `comparacao_modelos.json`: Rankings e mÃ©dias.
- `resultados.csv`: Planilha.

## ğŸ¤ ContribuiÃ§Ã£o

Issues e PRs bem-vindos! Para mudanÃ§as grandes, abra issue primeiro.

## ğŸ“„ LicenÃ§a

MIT License - veja LICENSE para detalhes.

---

**DÃºvidas?** Abra issue no GitHub ou consulte a documentaÃ§Ã£o do OpenRouter.
